== Kubernetes custom resource options
The cluster can be configured via a YAML file. This custom resource specifies the amount of replicas for each role group or role specific configuration like port definitions etc.

[source,yaml]
----
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkCluster
metadata:
  name: simple
spec:
  version: "3.0.1"
  config:
    logDir: /stackable/data/log
    enableMonitoring: true
  masters:
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        replicas: 1
        config:
          masterPort: 7078
          masterWebUiPort: 8081
        configOverrides:
          spark-defaults.conf:
            spark.driver.memory: 2g
          spark-env.sh:
            MASTER_ENV_VAR: test_value
  workers:
    roleGroups:
      2core2g:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        replicas: 1
        config:
          cores: 2
          memory: "2g"
          workerPort: 3031
          workerWebUiPort: 8083
        configOverrides:
          spark-defaults.conf:
            spark.driver.memory: 2g
          spark-env.sh:
            WORKER_ENV_VAR: test_value
  historyServers:
    roleGroups:
      default:
        selector:
          matchLabels:
            kubernetes.io/os: linux
        replicas: 1
        config:
          historyWebUiPort: 18081
        configOverrides:
          spark-defaults.conf:
            history.override: dummy_value
          spark-env.sh:
            HISTORY_ENV_VAR: test_value
----

== Configuration Overrides
Apache Spark runtime configuration is stored in a file named spark-defaults.conf. The configOverrides block allows you to add custom parameters to this file. A Full list of the available configuration options can be found in the official Apache Spark documentation at https://spark.apache.org/docs/latest/configuration.html.

Overrides consist of a key, which must match the property name in the configuration file and a value. This allows arbitrary configuration properties to be added to Spark. In the example above a dummy property spark.driver.memory is being explicitly set to '2g' for both the master and worker roles, overriding the default value.
